{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pytorch #\n",
    "## Torch의 텐서 라이브러리 ##\n",
    "\n",
    "모든 딥러닝은 텐서(Tensor)의 계산입니다. 텐서의 계산은 2차원 이상의 행렬 계산이라고 생각할 수 있습니다. 이것이 정확히 어떤 것을 의미하는지 나중에 알아보도록 하고..\n",
    "\n",
    "일단, 우리가 텐서를 통해 무엇을 할 수 있는지 알아볼 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2044ccb6438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author : Robert Guthrie\n",
    "# Translator : sweetcocoa\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 만들기 ##\n",
    "텐서는 파이썬 리스트로부터 torch.Tensor() 함수를 이용해서 만들 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  1  2\n",
      "  3  4\n",
      "\n",
      "(1 ,.,.) = \n",
      "  5  6\n",
      "  7  8\n",
      "[torch.FloatTensor of size 2x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 주어진 데이터로부터 텐서 오브젝트를 생성합니다.\n",
    "\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.Tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# 행렬을 만듭니다.\n",
    "M_data = [[1.,2.,3.], [4., 5., 6.]]\n",
    "M = torch.Tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# 2x2x2크기의 3차원 텐서를 만듭니다.\n",
    "T_data = [[[1., 2.], [3.,4.]], [[5., 6.], [7., 8.]]]\n",
    "T = torch.Tensor(T_data)\n",
    "print(T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D 텐서는 무엇일까요? 예를 들어, 벡터를 인덱싱한 원소는 스칼라 값이 됩니다. 행렬을 인덱싱한 원소는 벡터가 되지요. 마찬가지로 3D 텐서를 인덱싱한 원소는 행렬이 됩니다.\n",
    "\n",
    "**용어 정의** : 이 튜토리얼에서, '텐서' 혹은 'tensor' 라고 하는 것은 임의의 torch.Tensor 오브젝트를 말합니다. 만일 그 원소가 1차원 혹은 2차원의 데이터를 가지고 있더라도 마찬가지입니다. 만일 3D 텐서를 의미하는 말을 하고싶다면, '3D 텐서'라고 할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 다른 데이터 타입의 텐서 **\n",
    "\n",
    "물론 실수 데이터뿐 아니라 다른 데이터 타입의 텐서를 생성할 수 있습니다. 기본적으로는 앞에서 보셨듯이 실수(Float) 텐서가 만들어지지만, torch.LongTensor()를 사용하여 정수(Long)타입 텐서를 만들 수 있습니다. 그 외에도 다른 데이터타입이 있지만, 일단 가장 널리 쓰이는 것은 Float 텐서와 Long 텐서입니다.\n",
    "\n",
    "** 특정 차원의 랜덤 텐서 생성 **\n",
    "\n",
    "어떤 차원을 가진 랜덤 텐서를 torch.randn()함수를 통해 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -2.9718  1.7070 -0.4305 -2.2820  0.5237\n",
       "  0.0004 -1.2039  3.5283  0.4434  0.5848\n",
       "  0.8407  0.5510  0.3863  0.9124 -0.8410\n",
       "  1.2282 -1.8661  1.4146 -1.8781 -0.4674\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.7576  0.4215 -0.4827 -1.1198  0.3056\n",
       "  1.0386  0.5206 -0.5006  1.2182  0.2117\n",
       " -1.0613 -1.9441 -0.9596  0.5489 -0.9901\n",
       " -0.3826  1.5037  1.8267  0.5561  1.6445\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.4973 -1.5067  1.7661 -0.3569 -0.1713\n",
       "  0.4068 -0.4284 -1.1299  1.4274 -1.4027\n",
       "  1.4825 -1.1559  1.6190  0.9581  0.7747\n",
       "  0.1940  0.1687  0.3061  1.0743 -1.0327\n",
       "[torch.FloatTensor of size 3x4x5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,4,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 연산 ##\n",
    "\n",
    "여러분은 여러분이 생각하는 대로의 연산을 텐서 오브젝트에 적용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 5\n",
       " 7\n",
       " 9\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([1., 2., 3.])\n",
    "y = torch.Tensor([4.,5.,6.])\n",
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이외에도, [다른 연산(http://pytorch.org/docs/torch.html)](http://pytorch.org/docs/torch.html) 을 확인하실 수 있습니다. \n",
    "\n",
    "한 가지 유용한 연산은 **concatenation**(이어붙임) 인데, 한 번 살펴보도록 하죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0930  0.7769 -1.3128  0.7099  0.9944\n",
      "-0.2694 -0.6491 -0.1373 -0.2954 -0.7725\n",
      "-0.2215  0.5074 -0.6794 -1.6115  0.5230\n",
      "-0.8890  0.2620  0.0302  0.0013 -1.3987\n",
      " 1.4666 -0.1028 -0.0097 -0.8420 -0.2067\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      " 1.0672  0.1732 -0.6873  0.3620  0.3776 -0.2443 -0.5850  2.0812\n",
      " 0.3111  0.2358 -1.0658 -0.1186  0.4903  0.8349  0.8894  0.4148\n",
      "[torch.FloatTensor of size 2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 기본값으로, 첫 번째 축(0번째 축)을 기준으로 붙입니다. (2차원 행렬의 경우에는 가로축)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1]) # 5 x 5 행렬이 됩니다.\n",
    "print(z_1)\n",
    "\n",
    "# 반대로, 세로축(열) 을 기준으로 이을 수 있습니다.\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# 이 때는 기준 차원을 설정해줘야 합니다.\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 모양 변경(Reshape) ##\n",
    "\n",
    "텐서의 모양(shape)을 변경하기 위해서는 .view() 메소드를 사용합니다. 이 메소드는 굉장히 유용하고 자주 사용되니 알아두는 것이 좋겠습니다. 왜냐하면 많은 뉴럴네트워크 요소는 입력에 특정 차원 모양(shape)을 필요로 하기 때문이죠. 그래서 여러분은 뉴럴네트워크에 입력을 전달하기 전에 이 메소드를 사용해서 모양을 변경할 필요가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  0.0507 -0.9644 -2.0111  0.5245\n",
      "  2.1332 -0.0822  0.8388 -1.3233\n",
      "  0.0701  1.2200  0.4251 -1.2328\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.6195  1.5133  1.9954 -0.6585\n",
      " -0.4139 -0.2250 -0.6890  0.9882\n",
      "  0.7404 -2.0990  1.2582 -0.3990\n",
      "[torch.FloatTensor of size 2x3x4]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0507 -0.9644 -2.0111  0.5245  2.1332 -0.0822  0.8388 -1.3233  0.0701  1.2200\n",
      "-0.6195  1.5133  1.9954 -0.6585 -0.4139 -0.2250 -0.6890  0.9882  0.7404 -2.0990\n",
      "\n",
      "Columns 10 to 11 \n",
      " 0.4251 -1.2328\n",
      " 1.2582 -0.3990\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0507 -0.9644 -2.0111  0.5245  2.1332 -0.0822  0.8388 -1.3233  0.0701  1.2200\n",
      "-0.6195  1.5133  1.9954 -0.6585 -0.4139 -0.2250 -0.6890  0.9882  0.7404 -2.0990\n",
      "\n",
      "Columns 10 to 11 \n",
      " 0.4251 -1.2328\n",
      " 1.2582 -0.3990\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3,4)\n",
    "print(x)\n",
    "print(x.view(2,12)) # 2 x 12 텐서로 바꿉니다.\n",
    "print(x.view(2, -1)) # 한 차원의 값이 -1이면 그 축의 값은 추측됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 계산 그래프(Computaion Graphs) 와 자동 미분(Automatic Differentiation) ##\n",
    "\n",
    "**계산 그래프(Computation graph)** 라는 개념은 효율적인 딥 러닝 프로그래밍을 위해 필수적입니다. 우리가 직접 Back propagation gradient 알고리즘을 코드로 작성할 필요는 없으니까요. Computation graph는 간단히 말해 입력 데이터가 어떻게 결합하여 결과를 내는지를 나타낸 것입니다. \n",
    "\n",
    "그래프는 어떤 파라미터가 어떤 연산을 하는지를 나타내죠. 그러니까 그래프는 역전파 같은 알고리즘을 이용해 미분 연산을 하기에 적합한 정보를 갖고 있습니다... 는 약간 쓸모 없는 이야기처럼 들릴 지도 모르겠네요. 당연한 거 아닌가? 그래서 뭐가 어떻다는 이야기인지. 그러면 일단 Pytorch:autograd.Variable을 사용해서 무슨 일이 일어나는 지를 보면 좋을 것 같습니다.\n",
    "\n",
    "**첫째로,** 프로그래머의 입장에서 생각해보죠. 우리가 위에서 생성한 torch.Tensor 오브젝트에는 어떤 것이 저장되어 있나요? 물론 데이터와 모양(shape)이 있고, 그 외 다른 것들이 있을 수 있겠습니다. 그런데 만일 우리가 두 tensor를 더하면 한 tensor를 출력으로 갖게 되겠죠. 역시 마찬가지로 이것의 데이터와 모양(shape)도 알 수 있습니다. 그런데 출력만을 놓고 봤을 때, 우리는 이 tensor가 어떻게 생성되었는 지를 알 수 있나요? 이것이 파일로부터 온 것인지, 변수의 연산을 통해 온건지, 덧셈? 곱셈?을 통해 생성된 건지 알 수가 없습니다.\n",
    "\n",
    "**Variable Class**는 이러한 tensor가 어떻게 생성되었는지를 기록합니다. 한 번 보도록 하죠.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "<torch.autograd.function.AddBackward object at 0x000002044CD20BE8>\n"
     ]
    }
   ],
   "source": [
    "# Variable은 tensor 오브젝트를 wrapping합니다.\n",
    "x = autograd.Variable(torch.Tensor([1., 2., 3]), requires_grad=True)\n",
    "\n",
    "# Variable이 갖고 있는 데이터는 .data 애트리뷰트로 접근할 수 있습니다.\n",
    "print(x.data)\n",
    "\n",
    "# Tensor와 마찬가지로 Varaible에 대해서도 연산을 할 수 ㅇ있습니다.\n",
    "y = autograd.Variable(torch.Tensor([4., 5., 6]), requires_grad=True)\n",
    "z = x + y\n",
    "print(z.data)\n",
    "\n",
    "# 그런데 z는 뭔가 또 다른 것을 알고 있습니다. 무엇이 자신을 만들었는지요.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable은 그것이 file로부터 읽어지지 않았으며, 곱연산이나 지수연산 등등 다른 것에 의해 생성되지 않았다는 것 역시 알고 있습니다. 이것이 Tensor와 다른 점이죠. z.grad_fn를 따라가보면 여러분은 x, y 역시도 발견할 수 있을 것입니다.\n",
    "\n",
    "그런데 그래서 이게 어떻게 gradient를 계산하는 것을 도와준다는 거죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<torch.autograd.function.SumBackward object at 0x000002044CD20AF0>\n"
     ]
    }
   ],
   "source": [
    "# z의 모든 원소를 더해보겠습니다.\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, s라는 값에 대해 x의 첫번째 원소가 갖는 기울기는 얼마일까요? 즉 아래 값은 얼마일까요?\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{s}}{\\partial{x_0}}\n",
    "\\end{align}\n",
    "\n",
    "s는 자신이 z로부터 만들어진 것을 알고 있고, z는 자신이 x, y로부터 만들어진 것을 알고 있습니다. 다시 말하면, s는 자신이 x0, x1, x2, y0, y1, y2로부터 만들어진 것을 알고 있다는 뜻입니다. \n",
    "\n",
    "즉 s는 다음과 같은 공식을 포함하고 있습니다.\n",
    "\n",
    "\\begin{align}\n",
    "s = x_0 + y_0 + x_1 + y_1 + x_2 + y_2\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 우리가 원하던 값은 1이 된다는 것을 알 수 있죠!\n",
    "\n",
    "물론 이러한 설명은 내부적으로 실제로 어떻게 계산되는 지에 대한 설명으로는 부족합니다. 중요한 점은 s는 자신이 어떻게 계산되어왔는지를 알고 있다는 점이고, **Pytorch**의 개발자는 sum()과 + 계산에 대해 어떻게 미분값(기울기)를 계산할 지를 미리 정의해두고 그에 맞는 back propagtaion algorithm을 결정해두었다는 것이죠. 이러한 알고리즘의 자세한 기작은 너무 어려우니 잠깐 제쳐두고요.\n",
    "\n",
    "그럼 이제 Pytorch에게 기울기를 계산해보라고 해야겠군요. \n",
    "\n",
    "\n",
    "*참조 : 아래의 블록을 여러 번 실행하면, 기울기는 증가합니다. 이는 Pytorch가 기울기를 누적해서 연산하도록 .grad 값을내부적으로 정의해두었기 때문입니다. 많은 경우 이는 매우 편리한 기능입니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.backward() # 어떤 Variable 오브젝트의 .backward()를 호출하는 것으로 그 위치에서부터 backprop을 실행합니다.\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만일 여러분이 손실 함수로부터 오차를 구하기 위해서는, **절대로** 그 요소와 손실 함수 사이의 Variable chain을 끊지 않도록 주의해야 합니다. 만일 그렇다면, "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
