{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch로 딥러닝 하기 #\n",
    "\n",
    "\n",
    "## 딥러닝 블록 : Affine map, 비선형성과 Objective ##\n",
    "\n",
    "\n",
    "### Deep Learning Building Blocks: Affine maps, non-linearities and objectives ###\n",
    "\n",
    "\n",
    "딥 러닝은 선형성과 비선형성을 잘 연결하는 작업입니다. 비선형성의 도입은 강력한 모델을 만들 수 있게 해줍니다. 이 섹션에서는, 이 중요한 요소들을 이용해서 목적 함수를 만들고 어떻게 딥 러닝 모델이 훈련되는지 알아보겠습니다.\n",
    "\n",
    "## Affine maps ##\n",
    "\n",
    "\n",
    "딥 러닝에서 가장 유용한 도구중 하나는 **Affine map** 입니다. 이는 아래와 같은 함수를 말하는데요\n",
    "\\begin{align}\n",
    "f(x) = Ax + b\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**는 행렬, **x**, **b**는 벡터입니다. 학습되어야 하는 파라미터는 **A**와 **b**이고요. 보통 **b**는 *bias term* 이라고 합니다. \n",
    "\n",
    "PyTorch와 다른 딥러닝 프레임워크들은 전통적인 선형대수랑 약간 다른 일을 합니다. 선형대수는 열을 매핑하는 행렬을 찾지만, 딥러닝 프레임워크는 행벡터를 매핑하는 행렬을 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2e487d3c6a8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author : Robert Guthrie\n",
    "# 번역 : sweetcocoa\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3130  0.2576  1.3546\n",
      " 1.0007  0.6433  0.4951\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3) # R^5에서 R^3으로의 매핑을 말합니다. 파라미터 A, b를 포함합니다. \n",
    "\n",
    "data = autograd.Variable(torch.randn(2, 5))\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비선형성 (Non-Linearities) ##\n",
    "\n",
    "우리는 왜 비선형성이 필요할까요? 예를 들어봅시다. 만일 우리가 두 개의 Affine map, f = Ax + b, g = Cx + d 가 있다고 합시다. 그럼 두 map을 합성하면 아래와 같게 됩니다.\n",
    "\n",
    "\\begin{align}\n",
    "f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\n",
    "\\end{align}\n",
    "\n",
    "AC는 하나의 행렬, Ad + b는 하나의 벡터죠. 따라서 위의 합성 Affine map은 단지 하나의 Affine map일 뿐입니다. 하나의 Affine map 이상의 강력함을 갖지 못한다는 의미입니다. \n",
    "\n",
    "만일 우리가 두 가지 Affine map 사이에 비선형성을 추가할 수 있다면요? 그렇다면 합성 map은 더 이상 이전과 같지 않습니다. 더 강력한 모델을 생성할 수 있죠. \n",
    "\n",
    "몇 가지 중요한 비선형 함수가 있습니다. $\\tanh(x), \\sigma(x), \\text{ReLU}(x)$ (차례로 하이퍼볼릭 탄젠트, 시그모이드, 리엘유(렐루) 등으로 읽습니다)는 딥러닝에서 가장 널리 쓰이는 비선형 함수입니다. \n",
    "\n",
    "여러분은 하고 많은 비선형 함수들 중에 왜 하필 이 함수들인지 궁금할텐데요, 제 생각에는 이 함수들은 미분하기에 좋은 특성을 가지고 있기 때문입니다.\n",
    "\n",
    "\\begin{align}\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
