{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorch로 딥러닝하기 #\n",
    "\n",
    "## 딥 러닝의 기본 : Affine 매핑, 비선형성, 목적 함수 ##\n",
    "\n",
    "딥 러닝은 선형성과 비선형성을 잘 연결하는 작업입니다. 비선형성의 도입으로 더욱 강력한 딥 러닝 모델을 만들 수 있습니다. 이 섹션에서는, 이 요소들을 이용해서 목적 함수를 만들고, 이를 통해 어떻게 딥 러닝 모델이 훈련되는지 알아보겠습니다.\n",
    "\n",
    "\n",
    "## Affine 매핑(*Affine Maps*) ##\n",
    "\n",
    "딥 러닝의 가장 유용한 도구 중 하나는 **Affine map** 입니다. 이는 아래와 같은 함수 $f(x)$ 를 말합니다.\n",
    "\n",
    "\\begin{align}f(x) = Ax + b\\end{align}\n",
    "\n",
    "행렬 $A$ 와 벡터 $x, b$. 입니다. 학습되어야 하는 파라미터는 $A$ 와 $b$ 이고요. 보통 $b$를 *bias* 벡터 이라고 합니다.\n",
    "\n",
    "PyTorch와 다른 딥 러닝 프레임워크들은 전통적인 선형대수와는 약간 다른 연산을 합니다. 보통 선형대수는 열을 매핑하는데, 딥러닝 프레임워크들은 보통 행벡터를 매핑하는 행렬을 찾습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c5b3f243f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "# 번역 : Sweetcocoa\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3130  0.2576  1.3546\n",
      " 1.0007  0.6433  0.4951\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3)  # R^5에서 R^3으로의 매핑을 말합니다. 파라미터 A, b를 포함합니다. \n",
    "\n",
    "data = autograd.Variable(torch.randn(2, 5))\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linearities ##\n",
    "\n",
    "우리는 왜 비선형성이 필요할까요? 예를 들어서, 우리가 두 개의 affine maps, $f(x) = Ax + b$, $g(x) = Cx + d$ 를 합성하여 $f(g(x))$ 를 만든다고 생각해봅시다. 그러면 아래와 같이 됩니다.\n",
    "\n",
    "\\begin{align}f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\\end{align}\n",
    "\n",
    "$AC$ 는 하나의 행렬, $Ad + b$는 하나의 벡터죠.  따라서 단순히 affine map을 합성하는 것으로는 하나의 affine map을 넘는 무엇인가를 얻을 수 없습니다. \n",
    "\n",
    "만일 우리가 두 개 Affine map 사이에 비선형성을 추가할 수 있다면요?, 그렇다면 합성 map은 이전과는 다릅니다. 더욱 강력한 성능을 지닌 모델이 됩니다. 여기에 널리 쓰이는 비선형 함수에는 $\\tanh(x), \\sigma(x), \\text{ReLU}(x)$ 가 있습니다. 차례로 하이퍼볼릭 탄젠트, 시그모이드, 렐루(혹은 리엘유 등) 으로 읽습니다.\n",
    "\n",
    "여러분은 하고 많은 비선형 함수 중에 왜 하필 이 함수들인지 궁금하실텐데요, 제 생각에 이 함수들은 미분하기에 좋은 특성을 지니고 있기 때문입니다. 또 미분하여 함수의 기울기를 구하는 것은 학습에 있어 아주 필수적이고 중요한 연산이기 때문에 이러한 특성이 있는 비선형함수가 좋겠죠.\n",
    "\n",
    "시그모이드의 경우\n",
    "\\begin{align}\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\end{align}\n",
    "\n",
    "으로 좋은 미분 성질을 가지고 있습니다.\n",
    "\n",
    "*참고 사항* : 뉴럴 네트워크에서 $\\sigma(x)$를 비선형 함수로 사용하는 경우, 이론적으로는 큰 문제가 없지만 실제로 신경망에 적용하면 *gradient vanishing* 문제가 있습니다. 따라서 많은 경우 시그모이드 함수는 비선형 함수로 사용하지 않고, 대부분은 tanh나 ReLU, 혹은 그 변형 함수를 사용합니다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.2182  0.2117\n",
      "-1.0613 -1.9441\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 1.2182  0.2117\n",
      " 0.0000  0.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PyTorch에서는 대부분의 비선형 함수가 torch.functional 에 구현되어 있습니다.\n",
    "# 그리고 이러한 비선형 함수는 가중치(Weight)와 같이, affine map들이 갖는 파라미터를 갖지 않습니다. \n",
    "# 즉 학습 중에 업데이트 되지 않습니다.\n",
    "data = autograd.Variable(torch.randn(2, 2))\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and Probabilities\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The function $\\text{Softmax}(x)$ is also just a non-linearity, but\n",
    "it is special in that it usually is the last operation done in a\n",
    "network. This is because it takes in a vector of real numbers and\n",
    "returns a probability distribution. Its definition is as follows. Let\n",
    "$x$ be a vector of real numbers (positive, negative, whatever,\n",
    "there are no constraints). Then the i'th component of\n",
    "$\\text{Softmax}(x)$ is\n",
    "\n",
    "\\begin{align}\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\end{align}\n",
    "\n",
    "It should be clear that the output is a probability distribution: each\n",
    "element is non-negative and the sum over all components is 1.\n",
    "\n",
    "You could also think of it as just applying an element-wise\n",
    "exponentiation operator to the input to make everything non-negative and\n",
    "then dividing by the normalization constant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Softmax is also in torch.functional\n",
    "data = autograd.Variable(torch.randn(5))\n",
    "print(data)\n",
    "print(F.softmax(data))\n",
    "print(F.softmax(data).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Functions\n",
    "~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The objective function is the function that your network is being\n",
    "trained to minimize (in which case it is often called a *loss function*\n",
    "or *cost function*). This proceeds by first choosing a training\n",
    "instance, running it through your neural network, and then computing the\n",
    "loss of the output. The parameters of the model are then updated by\n",
    "taking the derivative of the loss function. Intuitively, if your model\n",
    "is completely confident in its answer, and its answer is wrong, your\n",
    "loss will be high. If it is very confident in its answer, and its answer\n",
    "is correct, the loss will be low.\n",
    "\n",
    "The idea behind minimizing the loss function on your training examples\n",
    "is that your network will hopefully generalize well and have small loss\n",
    "on unseen examples in your dev set, test set, or in production. An\n",
    "example loss function is the *negative log likelihood loss*, which is a\n",
    "very common objective for multi-class classification. For supervised\n",
    "multi-class classification, this means training the network to minimize\n",
    "the negative log probability of the correct output (or equivalently,\n",
    "maximize the log probability of the correct output).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization and Training\n",
    "=========================\n",
    "\n",
    "So what we can compute a loss function for an instance? What do we do\n",
    "with that? We saw earlier that autograd.Variable's know how to compute\n",
    "gradients with respect to the things that were used to compute it. Well,\n",
    "since our loss is an autograd.Variable, we can compute gradients with\n",
    "respect to all of the parameters used to compute it! Then we can perform\n",
    "standard gradient updates. Let $\\theta$ be our parameters,\n",
    "$L(\\theta)$ the loss function, and $\\eta$ a positive\n",
    "learning rate. Then:\n",
    "\n",
    "\\begin{align}\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\end{align}\n",
    "\n",
    "There are a huge collection of algorithms and active research in\n",
    "attempting to do something more than just this vanilla gradient update.\n",
    "Many attempt to vary the learning rate based on what is happening at\n",
    "train time. You don't need to worry about what specifically these\n",
    "algorithms are doing unless you are really interested. Torch provies\n",
    "many in the torch.optim package, and they are all completely\n",
    "transparent. Using the simplest gradient update is the same as the more\n",
    "complicated algorithms. Trying different update algorithms and different\n",
    "parameters for the update algorithms (like different initial learning\n",
    "rates) is important in optimizing your network's performance. Often,\n",
    "just replacing vanilla SGD with an optimizer like Adam or RMSProp will\n",
    "boost performance noticably.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Network Components in Pytorch\n",
    "======================================\n",
    "\n",
    "Before we move on to our focus on NLP, lets do an annotated example of\n",
    "building a network in Pytorch using only affine maps and\n",
    "non-linearities. We will also see how to compute a loss function, using\n",
    "Pytorch's built in negative log likelihood, and update parameters by\n",
    "backpropagation.\n",
    "\n",
    "All network components should inherit from nn.Module and override the\n",
    "forward() method. That is about it, as far as the boilerplate is\n",
    "concerned. Inheriting from nn.Module provides functionality to your\n",
    "component. For example, it makes it keep track of its trainable\n",
    "parameters, you can swap it between CPU and GPU with the .cuda() or\n",
    ".cpu() functions, etc.\n",
    "\n",
    "Let's write an annotated example of a network that takes in a sparse\n",
    "bag-of-words representation and outputs a probability distribution over\n",
    "two labels: \"English\" and \"Spanish\". This model is just logistic\n",
    "regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Logistic Regression Bag-of-Words classifier\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Our model will map a sparse BOW representation to log probabilities over\n",
    "labels. We assign each word in the vocab an index. For example, say our\n",
    "entire vocab is two words \"hello\" and \"world\", with indices 0 and 1\n",
    "respectively. The BoW vector for the sentence \"hello hello hello hello\"\n",
    "is\n",
    "\n",
    "\\begin{align}\\left[ 4, 0 \\right]\\end{align}\n",
    "\n",
    "For \"hello world world hello\", it is\n",
    "\n",
    "\\begin{align}\\left[ 2, 2 \\right]\\end{align}\n",
    "\n",
    "etc. In general, it is\n",
    "\n",
    "\\begin{align}\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\\end{align}\n",
    "\n",
    "Denote this BOW vector as $x$. The output of our network is:\n",
    "\n",
    "\\begin{align}\\log \\text{Softmax}(Ax + b)\\end{align}\n",
    "\n",
    "That is, we pass the input through an affine map and then do log\n",
    "softmax.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec))\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the Pytorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the above values corresponds to the log probability of ENGLISH,\n",
    "and which to SPANISH? We never defined it, but we need to if we want to\n",
    "train the thing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets train! To do this, we pass instances through to get log\n",
    "probabilities, compute a loss function, compute the gradient of the loss\n",
    "function, and then update the parameters with a gradient step. Loss\n",
    "functions are provided by Torch in the nn package. nn.NLLLoss() is the\n",
    "negative log likelihood loss we want. It also defines optimization\n",
    "functions in torch.optim. Here, we will just use SGD.\n",
    "\n",
    "Note that the *input* to NLLLoss is a vector of log probabilities, and a\n",
    "target label. It doesn't compute the log probabilities for us. This is\n",
    "why the last layer of our network is log softmax. The loss function\n",
    "nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log\n",
    "softmax for you.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run on test data before we train, just to see a before-and-after\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Variable as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the right answer! You can see that the log probability for\n",
    "Spanish is much higher in the first example, and the log probability for\n",
    "English is much higher in the second for the test data, as it should be.\n",
    "\n",
    "Now you see how to make a Pytorch component, pass some data through it\n",
    "and do gradient updates. We are ready to dig deeper into what deep NLP\n",
    "has to offer.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
